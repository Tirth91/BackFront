<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Gesture Video Call</title>
  <style>
    body { 
      background-color: #111; 
      color: white; 
      font-family: Arial, sans-serif; 
      margin: 0; 
      padding: 20px;
      display: flex;
      flex-direction: column;
      min-height: 100vh;
    }
    h1 { 
      color: #0f0; 
      text-align: center;
      margin-bottom: 10px;
    }
    #status { 
      color: #0f0; 
      margin: 10px;
      text-align: center;
    }
    .controls {
      display: flex;
      justify-content: center;
      gap: 10px;
      margin-bottom: 20px;
    }
    button {
      background: #0f0; 
      color: #111; 
      border: none; 
      padding: 10px 20px; 
      font-weight: bold; 
      border-radius: 5px; 
      cursor: pointer;
      transition: background 0.3s;
    }
    button:hover { background: #0c0; }
    button:disabled {
      background: #333;
      color: #666;
      cursor: not-allowed;
    }
    #video-container {
      flex: 1;
      display: flex;
      flex-direction: column;
    }
    #video-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
      gap: 20px;
      margin: 0 auto;
      width: 100%;
      max-width: 1200px;
    }
    .video-box {
      position: relative;
      background: #000;
      border-radius: 10px;
      overflow: hidden;
      aspect-ratio: 4/3;
      box-shadow: 0 0 10px rgba(0, 255, 0, 0.2);
    }
    .video-box video, .video-box .video-stream {
      width: 100%; 
      height: 100%; 
      object-fit: cover;
    }
    .prediction-label {
      position: absolute;
      bottom: 0; 
      left: 0; 
      width: 100%; 
      background: rgba(0,0,0,0.7);
      color: #0f0; 
      padding: 10px; 
      font-size: 1rem;
      text-align: center;
      font-weight: bold;
    }
    .user-name {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      background: rgba(0,0,0,0.7);
      color: white;
      padding: 5px;
      font-size: 0.9rem;
      text-align: center;
    }
    #hidden-cam { display: none; }
    #room-id-container {
      text-align: center;
      margin-bottom: 20px;
    }
    #room-id-input {
      padding: 8px;
      border-radius: 5px;
      border: 1px solid #0f0;
      background: #222;
      color: white;
      margin-right: 10px;
      width: 200px;
    }
    #participant-count {
      text-align: center;
      margin-top: 10px;
      color: #0f0;
    }
  </style>
</head>
<body>
  <h1>üñêÔ∏è Gesture Video Call</h1>
  <div id="room-id-container">
    <input type="text" id="room-id-input" placeholder="Enter room ID" value="test">
    <button onclick="joinCall()">Join Call</button>
    <button id="leave-btn" onclick="leaveCall()" disabled>Leave Call</button>
  </div>
  <div id="status">Ready to join</div>
  <div id="participant-count">Participants: 0</div>
  
  <div id="video-container">
    <div id="video-grid"></div>
  </div>

  <video id="hidden-cam" autoplay playsinline muted></video>

  <!-- Libraries -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.10.0"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
  <script src="https://download.agora.io/sdk/release/AgoraRTC_N.js"></script>

  <!-- App Script -->
  <script>
    const APP_ID = "0f3fde8ae17c4048bcfc8d69286bc851";
    const DEFAULT_CHANNEL = "test";
    const labelMap = ["1L","1R","2L","2R","3L","3R","4L","4R","5R","6L","6R","7L","7R","8L","8R","9L","9R","A","B","C","D","L"];
    const CONFIDENCE_THRESHOLD = 0.7;
    const PREDICTION_INTERVAL = 500; // ms between predictions to reduce load

    const client = AgoraRTC.createClient({ mode: "rtc", codec: "vp8" });
    const videoGrid = document.getElementById("video-grid");
    const status = document.getElementById("status");
    const hiddenVideo = document.getElementById("hidden-cam");
    const participantCount = document.getElementById("participant-count");
    const leaveBtn = document.getElementById("leave-btn");
    const roomIdInput = document.getElementById("room-id-input");

    let model, localTrack, dataStreamId, predictionInterval;
    let participants = new Set();
    let lastPredictionTime = 0;

    // Initialize MediaPipe Hands
    const hands = new Hands({ 
      locateFile: file => `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}` 
    });
    hands.setOptions({ 
      maxNumHands: 2, 
      modelComplexity: 1, 
      minDetectionConfidence: 0.7, 
      minTrackingConfidence: 0.7 
    });
    hands.onResults(onResults);

    async function joinCall() {
      const CHANNEL = roomIdInput.value.trim() || DEFAULT_CHANNEL;
      
      if (!CHANNEL) {
        status.textContent = "Please enter a room ID";
        return;
      }

      status.textContent = "Loading model...";
      try {
        model = await tf.loadLayersModel("landmark_model_tfjs/model.json");
      } catch (error) {
        status.textContent = "Failed to load model";
        console.error("Model loading error:", error);
        return;
      }

      status.textContent = "Starting camera...";
      let stream;
      try {
        stream = await navigator.mediaDevices.getUserMedia({ 
          video: { width: 640, height: 480, facingMode: "user" },
          audio: false
        });
      } catch (error) {
        status.textContent = "Camera access denied";
        console.error("Camera error:", error);
        return;
      }

      hiddenVideo.srcObject = stream;
      const cam = new Camera(hiddenVideo, { 
        onFrame: async () => {
          const now = Date.now();
          if (now - lastPredictionTime > PREDICTION_INTERVAL) {
            await hands.send({ image: hiddenVideo });
            lastPredictionTime = now;
          }
        }, 
        width: 640, 
        height: 480 
      });
      cam.start();

      status.textContent = "Joining call...";
      try {
        await client.join(APP_ID, CHANNEL, null, null);
        participants.add("local");
        updateParticipantCount();
        
        // Create and publish local video track
        localTrack = await AgoraRTC.createCameraVideoTrack({
          encoderConfig: "480p_4",
          optimizationMode: "detail"
        });
        createVideoBox("local", "You");
        localTrack.play("local");
        await client.publish([localTrack]);
        
        // Create data channel for sending gesture predictions
        dataStreamId = await client.createDataStream({ reliable: true, ordered: true });

        // Setup event listeners
        setupEventListeners();
        
        leaveBtn.disabled = false;
        roomIdInput.disabled = true;
        status.textContent = `In call: ${CHANNEL}`;
      } catch (error) {
        status.textContent = "Failed to join call";
        console.error("Join error:", error);
        if (stream) {
          stream.getTracks().forEach(track => track.stop());
        }
      }
    }

    function setupEventListeners() {
      // When a remote user publishes a stream
      client.on("user-published", async (user, mediaType) => {
        await client.subscribe(user, mediaType);
        if (mediaType === "video") {
          const id = `remote-${user.uid}`;
          participants.add(id);
          updateParticipantCount();
          createVideoBox(id, `User ${user.uid}`);
          user.videoTrack.play(id);
        }
      });

      // When a remote user leaves
      client.on("user-unpublished", (user) => {
        const id = `remote-${user.uid}`;
        participants.delete(id);
        updateParticipantCount();
        removeVideoBox(id);
      });

      // When a remote user leaves the channel
      client.on("user-left", (user) => {
        const id = `remote-${user.uid}`;
        participants.delete(id);
        updateParticipantCount();
        removeVideoBox(id);
      });

      // When receiving gesture prediction data
      client.on("stream-message", (uid, message) => {
        const label = document.getElementById(`label-remote-${uid}`);
        if (label) {
          label.textContent = `${label.textContent.split(':')[0]}: ${message}`;
        }
      });
    }

    function createVideoBox(id, name) {
      // Don't create if already exists
      if (document.getElementById(`box-${id}`)) return;

      const box = document.createElement("div");
      box.className = "video-box";
      box.id = `box-${id}`;

      const stream = document.createElement("div");
      stream.className = "video-stream";
      stream.id = id;

      const nameLabel = document.createElement("div");
      nameLabel.className = "user-name";
      nameLabel.textContent = name;

      const predictionLabel = document.createElement("div");
      predictionLabel.className = "prediction-label";
      predictionLabel.id = `label-${id}`;
      predictionLabel.textContent = "No gesture detected";

      box.appendChild(stream);
      box.appendChild(nameLabel);
      box.appendChild(predictionLabel);
      videoGrid.appendChild(box);
    }

    function removeVideoBox(id) {
      const box = document.getElementById(`box-${id}`);
      if (box) {
        box.remove();
      }
    }

    function updateParticipantCount() {
      participantCount.textContent = `Participants: ${participants.size}`;
    }

    function onResults(results) {
      if (!model) return;
      
      // Extract landmarks from hand detection results
      const landmarks = [];
      for (let i = 0; i < 2; i++) {
        if (results.multiHandLandmarks[i]) {
          for (let lm of results.multiHandLandmarks[i]) {
            landmarks.push(lm.x, lm.y, lm.z);
          }
        } else {
          for (let j = 0; j < 21; j++) landmarks.push(0, 0, 0);
        }
      }
      
      // Pad with zeros if needed
      while (landmarks.length < 188) landmarks.push(0);
      
      // Only predict if we have some non-zero landmarks
      if (landmarks.some(v => v !== 0)) {
        const input = tf.tensor2d([landmarks]);
        const prediction = model.predict(input);
        
        prediction.array().then(data => {
          const maxVal = Math.max(...data[0]);
          const maxIndex = data[0].indexOf(maxVal);
          const label = maxVal >= CONFIDENCE_THRESHOLD ? labelMap[maxIndex] : "No gesture";
          
          // Update local display
          document.getElementById("label-local").textContent = `You: ${label}`;
          
          // Send prediction to other participants
          if (dataStreamId) {
            client.sendStreamMessage(dataStreamId, label);
          }
        }).catch(err => {
          console.error("Prediction error:", err);
        }).finally(() => {
          // Clean up tensors to avoid memory leaks
          input.dispose();
          prediction.dispose();
        });
      }
    }

    async function leaveCall() {
      try {
        await client.leave();
        
        // Clean up local tracks
        if (localTrack) {
          localTrack.stop();
          localTrack.close();
        }
        
        // Stop hidden camera stream
        const hiddenCamStream = hiddenVideo.srcObject;
        if (hiddenCamStream) {
          hiddenCamStream.getTracks().forEach(track => track.stop());
          hiddenVideo.srcObject = null;
        }
        
        // Clear UI
        videoGrid.innerHTML = "";
        participants.clear();
        updateParticipantCount();
        
        // Reset UI state
        leaveBtn.disabled = true;
        roomIdInput.disabled = false;
        status.textContent = "Left call";
      } catch (error) {
        console.error("Leave error:", error);
        status.textContent = "Error leaving call";
      }
    }

    // Clean up on page unload
    window.addEventListener('beforeunload', leaveCall);
  </script>
</body>
</html>